{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb047d-b0a8-4ac9-a0f3-1e34633e72da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "\tTrain Loss: 1.56845 | Train PPL: 4.79922 | task runs 20013.717 seconds | epoch 0\n",
      "\t Val. Loss: 1.40822 |  Val. PPL: 4.08865 | task runs 20013.717 seconds | epoch 0\n",
      "\tTrain Loss: 1.39011 | Train PPL: 4.01530 | task runs 20016.917 seconds | epoch 1\n",
      "\t Val. Loss: 1.31343 |  Val. PPL: 3.71892 | task runs 20016.917 seconds | epoch 1\n",
      "\tTrain Loss: 1.32654 | Train PPL: 3.76797 | task runs 20026.169 seconds | epoch 2\n",
      "\t Val. Loss: 1.27523 |  Val. PPL: 3.57952 | task runs 20026.169 seconds | epoch 2\n",
      "\tTrain Loss: 1.29855 | Train PPL: 3.66398 | task runs 20017.582 seconds | epoch 3\n",
      "\t Val. Loss: 1.25757 |  Val. PPL: 3.51688 | task runs 20017.582 seconds | epoch 3\n",
      "\tTrain Loss: 1.28155 | Train PPL: 3.60222 | task runs 20048.054 seconds | epoch 4\n",
      "\t Val. Loss: 1.24651 |  Val. PPL: 3.47817 | task runs 20048.054 seconds | epoch 4\n",
      "\tTrain Loss: 1.26890 | Train PPL: 3.55694 | task runs 13805.829 seconds | epoch 5\n",
      "\t Val. Loss: 1.23706 |  Val. PPL: 3.44546 | task runs 13805.829 seconds | epoch 5\n",
      "\tTrain Loss: 1.25897 | Train PPL: 3.52181 | task runs 11992.632 seconds | epoch 6\n",
      "\t Val. Loss: 1.22881 |  Val. PPL: 3.41717 | task runs 11992.632 seconds | epoch 6\n",
      "\tTrain Loss: 1.25018 | Train PPL: 3.49097 | task runs 11996.497 seconds | epoch 7\n",
      "\t Val. Loss: 1.22423 |  Val. PPL: 3.40155 | task runs 11996.497 seconds | epoch 7\n",
      "\tTrain Loss: 1.24315 | Train PPL: 3.46651 | task runs 12000.812 seconds | epoch 8\n",
      "\t Val. Loss: 1.22221 |  Val. PPL: 3.39467 | task runs 12000.812 seconds | epoch 8\n",
      "\tTrain Loss: 1.23717 | Train PPL: 3.44585 | task runs 11993.884 seconds | epoch 9\n",
      "\t Val. Loss: 1.21757 |  Val. PPL: 3.37896 | task runs 11993.884 seconds | epoch 9\n",
      "\tTrain Loss: 1.23141 | Train PPL: 3.42604 | task runs 11998.268 seconds | epoch 10\n",
      "\t Val. Loss: 1.21655 |  Val. PPL: 3.37551 | task runs 11998.268 seconds | epoch 10\n",
      "\tTrain Loss: 1.22658 | Train PPL: 3.40956 | task runs 11995.453 seconds | epoch 11\n",
      "\t Val. Loss: 1.21336 |  Val. PPL: 3.36479 | task runs 11995.453 seconds | epoch 11\n",
      "\tTrain Loss: 1.22125 | Train PPL: 3.39141 | task runs 11997.554 seconds | epoch 12\n",
      "\t Val. Loss: 1.21206 |  Val. PPL: 3.36039 | task runs 11997.554 seconds | epoch 12\n",
      "\tTrain Loss: 1.21741 | Train PPL: 3.37841 | task runs 11996.339 seconds | epoch 13\n",
      "\t Val. Loss: 1.20900 |  Val. PPL: 3.35013 | task runs 11996.339 seconds | epoch 13\n",
      "\tTrain Loss: 1.21275 | Train PPL: 3.36272 | task runs 11997.554 seconds | epoch 14\n",
      "\t Val. Loss: 1.20963 |  Val. PPL: 3.35225 | task runs 11997.554 seconds | epoch 14\n",
      "\tTrain Loss: 1.20880 | Train PPL: 3.34945 | task runs 12002.493 seconds | epoch 15\n",
      "\t Val. Loss: 1.20433 |  Val. PPL: 3.33453 | task runs 12002.493 seconds | epoch 15\n",
      "\tTrain Loss: 1.20484 | Train PPL: 3.33623 | task runs 12001.613 seconds | epoch 16\n",
      "\t Val. Loss: 1.20557 |  Val. PPL: 3.33865 | task runs 12001.613 seconds | epoch 16\n",
      "\tTrain Loss: 1.20118 | Train PPL: 3.32405 | task runs 12003.644 seconds | epoch 17\n",
      "\t Val. Loss: 1.20659 |  Val. PPL: 3.34205 | task runs 12003.644 seconds | epoch 17\n",
      "\tTrain Loss: 1.19715 | Train PPL: 3.31067 | task runs 12002.818 seconds | epoch 18\n",
      "\t Val. Loss: 1.20427 |  Val. PPL: 3.33434 | task runs 12002.818 seconds | epoch 18\n",
      "\tTrain Loss: 1.19352 | Train PPL: 3.29867 | task runs 12005.533 seconds | epoch 19\n",
      "\t Val. Loss: 1.20564 |  Val. PPL: 3.33888 | task runs 12005.533 seconds | epoch 19\n",
      "\tTrain Loss: 1.19035 | Train PPL: 3.28824 | task runs 12002.876 seconds | epoch 20\n",
      "\t Val. Loss: 1.20438 |  Val. PPL: 3.33469 | task runs 12002.876 seconds | epoch 20\n",
      "\tTrain Loss: 1.18630 | Train PPL: 3.27493 | task runs 12005.240 seconds | epoch 21\n",
      "\t Val. Loss: 1.20436 |  Val. PPL: 3.33464 | task runs 12005.240 seconds | epoch 21\n",
      "\tTrain Loss: 1.18352 | Train PPL: 3.26586 | task runs 12000.312 seconds | epoch 22\n",
      "\t Val. Loss: 1.20629 |  Val. PPL: 3.34105 | task runs 12000.312 seconds | epoch 22\n",
      "\tTrain Loss: 1.17973 | Train PPL: 3.25348 | task runs 11999.326 seconds | epoch 23\n",
      "\t Val. Loss: 1.20426 |  Val. PPL: 3.33430 | task runs 11999.326 seconds | epoch 23\n"
     ]
    }
   ],
   "source": [
    "# 按照 seq2seq的paper所说 增加了 bidirectional 翻转，同时，layer深层时效果比浅层的要好，所以 layer=4\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "# 变长序列的处理\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence,pad_packed_sequence\n",
    "from visdom import Visdom\n",
    "import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "beatMod = 4\n",
    "g_AttrIndex = [] # 谱面notes中信息的映射\n",
    "\n",
    "testId = 6 # 全部数据\n",
    "# testId = 7  # 10 数据 去掉dropout 想办法搞成过拟合\n",
    "# testId = 8 # 500数据\n",
    "modelId = 31\n",
    "bVisdom = False\n",
    "\n",
    "splitPercent = 0.95\n",
    "N_EPOCHS = 50\n",
    "de_feature_size = 256\n",
    "hidden_dim = 512\n",
    "enc_hid_dim = 512\n",
    "dec_hid_dim = 512\n",
    "n_layers = 4\n",
    "learning_rate = 0.00001\n",
    "batch_size = 1\n",
    "dropout = 0.1\n",
    "teacher_forcing_ratio = 0.8\n",
    "PosibleKind = 433\n",
    "output_size = 433  \n",
    "seq_size = 64         # 一句话的长度\n",
    "en_feature_size = 32  # 音频标识数组尺寸\n",
    "bidirectional = True # 是否翻转\n",
    "\n",
    "with open(\"./datasetAll\" + str(testId) + \".json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "X = dataset['X']\n",
    "Y = dataset['Y']\n",
    "\n",
    "class GetLoader(Dataset):\n",
    "    def __init__(self, data_root, data_label):\n",
    "        self.data = data_root\n",
    "        self.label = data_label\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        labels = self.label[index]\n",
    "        return data, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def get_Data():\n",
    "    # 获取分割比例\n",
    "    train_index = int(len(X) * splitPercent)\n",
    "    \n",
    "    train_x = []\n",
    "    for i in range (0, train_index):\n",
    "        train_x.append([X[i][j] for j in range (0, seq_size)])\n",
    "    train_x = torch.FloatTensor(train_x)\n",
    "    \n",
    "    valid_x = []\n",
    "    for i in range (train_index, len(X)):\n",
    "        valid_x.append([X[i][j] for j in range (0, seq_size)])\n",
    "    valid_x = torch.FloatTensor(valid_x)\n",
    "    \n",
    "    train_y = []\n",
    "    for i in range (0, train_index):\n",
    "        train_y.append([Y[i][j] for j in range (0, seq_size)])\n",
    "    train_y = torch.FloatTensor(train_y)\n",
    "\n",
    "    valid_y = []\n",
    "    for i in range (train_index, len(Y)):\n",
    "        valid_y.append([Y[i][j] for j in range (0, seq_size)])\n",
    "    valid_y = torch.FloatTensor(valid_y)\n",
    "    \n",
    "    train_data = GetLoader(train_x, train_y)\n",
    "    valid_data = GetLoader(valid_x, valid_y)\n",
    "    \n",
    "    return (\n",
    "        DataLoader(train_data, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(valid_data, batch_size=batch_size, shuffle=False),\n",
    "    )\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, n_layers, dropout):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(feature_size, hidden_size, n_layers, bidirectional = bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hid_dim = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "    # def forward(self, src, hidden):\n",
    "    #     out, h = self.gru(src, hidden)\n",
    "    #     return out, h\n",
    "    \n",
    "    def combine_bidir(self, outs, bsz: int):\n",
    "        out = outs.view(self.n_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
    "        return out.view(self.n_layers, bsz, -1)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        out, hidden = self.gru(src)\n",
    "        # hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        b_size = src.shape[1]\n",
    "        hidden = self.combine_bidir(hidden, b_size)\n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "        return out, hidden\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, output_size, n_layers, dropout):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hid_dim = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.output_dim = output_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(de_feature_size, hidden_size, n_layers)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.dropout(self.embedding(input)).to(device)\n",
    "        embedded = F.relu(embedded)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        # [seq,batch_size,hidden_size]\n",
    "        prediction = self.fc_out(output)\n",
    "        # prediction = self.softmax(prediction)\n",
    "        return prediction, hidden\n",
    "        \n",
    "\n",
    "def load_word_embeddings(file_name, dim):\n",
    "    term_ids = {}\n",
    "    we_matrix = []\n",
    "    term_ids['NULL'] = 0\n",
    "    term_by_id = ['NULL']\n",
    "    we_matrix.append([0] * dim)\n",
    "    term_num = 1\n",
    "    with open(file_name) as FileObj:\n",
    "        for line in FileObj:\n",
    "            line = line.split()\n",
    "            term_ids[line[0].strip()] = term_num\n",
    "            term_by_id.append(line[0].strip())\n",
    "            norm = 1\n",
    "            we_matrix.append([float(i) / norm for i in line[-de_feature_size:]])\n",
    "            term_num += 1\n",
    "    return term_ids, term_by_id, we_matrix\n",
    "\n",
    "\n",
    "def get_glove_embedding(classes, filename):\n",
    "    term_to_id, id_to_term, we_matrix = load_word_embeddings(\"glove/jsonfile\" + str(testId) + \"/\" + filename, de_feature_size)\n",
    "    embedding_matrix = np.random.rand(classes, de_feature_size)\n",
    "    for i in range(classes):\n",
    "        if str(i) in term_to_id:\n",
    "            tid = term_to_id[str(i)]\n",
    "            embedding_matrix[i] = we_matrix[tid]\n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = teacher_forcing_ratio):\n",
    "        b_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        _, hidden = self.encoder(src)\n",
    "        # print(f\"hidden.shape {hidden.shape}\") # hidden.shape torch.Size([1, 1, 512])\n",
    "        # src_len = src.shape[0]\n",
    "        # hidden = torch.zeros(n_layers, b_size, hidden_dim).to(device)\n",
    "        # for t in range(1, src_len):\n",
    "        #     input_en = src[t].unsqueeze(0).to(device)\n",
    "        #     _, hidden = self.encoder(input_en, hidden)\n",
    "        \n",
    "        # 正交初始化\n",
    "        outputs = torch.zeros(trg_len, b_size, trg_vocab_size).to(self.device)\n",
    "        nn.init.orthogonal_(outputs)\n",
    "        # outputs = torch.rand(trg_len, b_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        trgInput = trg[0]\n",
    "        for t in range(1, trg_len):\n",
    "            trgInput = trgInput.unsqueeze(0).to(device)\n",
    "            \n",
    "            pred, hidden = self.decoder(trgInput, hidden)\n",
    "            pred = pred.squeeze(0).to(device)\n",
    "            outputs[t] = pred\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            trgInput = trg[t] if teacher_force else pred.argmax(1)\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(get_glove_embedding(PosibleKind, \"vectors.txt\"), freeze=False).to(device)\n",
    "train_iter, valid_iter = get_Data()\n",
    "encoder = EncoderRNN(en_feature_size, hidden_dim, n_layers, dropout).to(device)\n",
    "decoder = DecoderRNN(embedding, hidden_dim, output_size, n_layers, dropout).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        src = batch[0]\n",
    "        trg = batch[1].long()\n",
    "        src = src.transpose(0,1).to(device)\n",
    "        trgInput = trg.transpose(0,1).to(device)\n",
    "        # print(f\" src.shape : { src.shape }  trgInput.shape : { trgInput.shape }\")\n",
    "        output = model(src, trgInput) \n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:].view(-1, output_dim).to(device)\n",
    "        trg = trg[:].view(-1).to(device)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # print(f'train epoch_loss / len(iterator): {epoch_loss / len(iterator)}') \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0]\n",
    "            trg = batch[1].long()\n",
    "            src = src.transpose(0,1).to(device)\n",
    "            trgInput = trg.transpose(0,1).to(device)\n",
    "            output = model(src, trgInput) \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:].view(-1, output_dim).to(device)\n",
    "            trg = trg[:].view(-1).to(device)\n",
    "            loss = criterion(output, trg).to(device)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    # print(f'evaluate epoch_loss / len(iterator): {epoch_loss / len(iterator)}') \n",
    "    return epoch_loss / len(iterator)\n",
    "    \n",
    "import math, time\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if bVisdom:\n",
    "    viz = Visdom() \n",
    "    viz.line([[0., 0.]], [0], win='train_valid_loss' + str(modelId), opts=dict(title='train_valid_loss' + str(modelId)))\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    startTime = time.time()\n",
    "    train_loss = train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "    endTime = time.time()\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut1-model\" + str(modelId) + \".pt\")\n",
    "\n",
    "    if bVisdom:\n",
    "        viz.line([[valid_loss], [train_loss]], [epoch], win='train_valid_loss' + str(modelId), update='append')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f} | Train PPL: {math.exp(train_loss):7.5f} | task runs {(endTime - startTime):.3f} seconds | epoch {epoch}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. PPL: {math.exp(valid_loss):7.5f} | task runs {(endTime - startTime):.3f} seconds | epoch {epoch}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
